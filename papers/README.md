# Paper Review
## Preprocessing

- An Empirical Comparison of Machine Learning Models for Time Series Forecasting(2010)
  - https://www.researchgate.net/publication/227612766_An_Empirical_Comparison_of_Machine_Learning_Models_for_Time_Series_Forecasting
  > PREPROCESSING METHODS

<br><br><br>

---

## Model

- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation(Jun 2014)
  - https://arxiv.org/abs/1406.1078
  > RNN Encoderâ€“Decoder
- Sequence to Sequence Learning with Neural Networks(Sep 2014)
  - https://arxiv.org/abs/1409.3215
  > Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori. For example, speech recognition and machine translation are sequential problems. Likewise, question answering can also be seen as mapping a sequence of words representing the question to a 1 sequence of words representing the answer. It is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful.
- Neural Networks for Time Series Processing(1996)
  - https://www.semanticscholar.org/paper/Neural-Networks-for-Time-Series-Processing-Dorffner/b9a1c422c29b8e4dfb1a9c91d426fc3894726e88
  > It underlined one important contribution of neural networks namely their elegant ability to approximate arbitrary nonlinear functions This property is of high value in time series processing and promises more powerful applications especially in the sub-field of fore casting in the near future However it was also emphasized that nonlinear models are not without problems both with respect to their requirement for large data bases and careful evaluation and with respect to limitations of learning or of learning or estimation algorithms.
